{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "# Copyright 2025 Google LLC\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "```"
      ],
      "metadata": {
        "id": "akanYvxE3XpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube AI Event Detector\n",
        "\n",
        "This Colab notebook is designed to analyze YouTube videos from a specified channel and determine if they contain a user-defined element. It leverages the power of the [Vertex API](https://cloud.google.com/vertex-ai/docs) to analyze video content and the [YouTube Data API](https://developers.google.com/youtube/v3/docs) to retrieve video metadata. The script outputs a structured report indicating the presence of the element and, if present, the approximate time range where it appears.\n",
        "\n",
        "\u003cbr/\u003e\n",
        "\n",
        "## Requirements\n",
        "\n",
        "In order to use this Colab you will need the following information:\n",
        "\n",
        "  1. Your channel ID. This is a 24 character ID that you can find in the URL of your channel.\n",
        "\n",
        "  2. Create a Google Cloud Project if you don't have one using the [following tutorial](https://developers.google.com/workspace/guides/create-project).\n",
        "\n",
        "  3. Enable the YouTube APIs and Vertex API on of the [Google Cloud Console](https://support.google.com/googleapi/answer/6158841?hl=en).\n",
        "\n",
        "  4. Create a service account in IAM section and a service account key for it using the [following tutorial](https://cloud.google.com/iam/docs/keys-create-delete).\n",
        "\n",
        "  5. Download the service key that you created in step 4 in and keep it a safe place. This file will be used to authenticate this script to your account.\n",
        "\n",
        "\n",
        "\u003e **NOTE:** Vertex AI API is a paid feature and requires a billing account linked to your project. More [here](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "\u003cbr/\u003e\n",
        "\n",
        "## What does the script do?\n",
        "\n",
        "The script performs the following actions:\n",
        "\n",
        "* Retrieves a list of videos from a specified YouTube channel, sorted by view count.\n",
        "* Analyzes each video using the Gemini API to detect the presence of a user-specified element.\n",
        "* Records the analysis results, including the presence of the element and its time range (if applicable).\n",
        "* Saves the results to a CSV file in a user-specified Google Drive folder.\n",
        "Includes error handling with retry logic to manage potential API issues.\n",
        "\n",
        "\u003cbr/\u003e\n",
        "\n",
        "## What the script is GOOD to detect?\n",
        "\n",
        "Elements that have **physical** presence or that you can define by **nouns** are good candidates to be detected. Also, **situations** (where someone performs an action) can be easily identified by Gemini. The more specific you are, the better results you get.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Someone wearing a black T-shirt\n",
        "* A dog barking\n",
        "* Drone footage\n",
        "* A sunset\n",
        "\n",
        "\u003cbr/\u003e\n",
        "\n",
        "\n",
        "## What the script is NOT SO GOOD to detect?\n",
        "\n",
        "Elements that are **subjective** or **open to interpretation**.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* A person making a mistake\n",
        "* Someone happy/sad/afraid\n",
        "* A beautiful sunset\n"
      ],
      "metadata": {
        "id": "NKkbdA-OZBxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "The script uses several configuration variables that need to be set by the user:\n",
        "\n",
        "* `ROOT_FOLDER_NAME`: The name of the folder in Google Drive where the output CSV file will be saved. If the folder doesn't exist, one with this name will be created.\n",
        "* `CSV_NAME`: The name you want to give to the output CSV file.\n",
        "* `CHANNEL_ID`: The ID of the YouTube channel to analyze.\n",
        "* `PROJECT_ID`: The ID of your Google Cloud Project. You can find your project ID following [these steps](https://support.google.com/googleapi/answer/7014113?hl=en).\n",
        "* `LOCATION`: The location of your Google Cloud Project.\n",
        "* `VIDEO_COUNT`: The number of videos to analyze, chosen from predefined options (20, 50, or 100).\n",
        "* `ELEMENT_TO_FIND`: The specific element to search for within the videos (e.g., \"a cat,\" \"a product demonstration\").\n",
        "* `HARD_LIMIT`: A hard limit on the number of videos to retrieve from the channel, ordered by the published date.\n",
        "* `FINAL_PROMPT`: A formatted prompt that is sent to the Gemini API to analyze the video. It instructs Gemini on what to look for and the desired output format."
      ],
      "metadata": {
        "id": "Y4wy-IfxOLC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_FOLDER_NAME = \"ElementChecker\" #@param {type:\"string\"}\n",
        "CSV_NAME = \"\" #@param {type:\"string\"}\n",
        "CHANNEL_ID = \"\" #@param {type:\"string\"}\n",
        "PROJECT_ID =  \"\" #@param {type:\"string\"}\n",
        "LOCATION = \"\" #@param {type:\"string\"}\n",
        "VIDEO_COUNT = 100 #@param [20, 50, 100]\n",
        "ELEMENT_TO_FIND = \"\" #@param {type:\"string\"}\n",
        "HARD_LIMIT = 5000 #@param {type:\"integer\"}\n",
        "\n",
        "## PROMPT\n",
        "FINAL_PROMPT = f\"\"\"\n",
        "Given the following video, can you answer if it contains {ELEMENT_TO_FIND}?\n",
        "If yes, could you provide the aproximate time range where it appears?\n",
        "I'd like to output a json object with the following format:\n",
        "{{\n",
        "  has_element: TRUE, if the element is present. FALSE otherwise.\n",
        "  details: The aproximate time range where the element appears If the has_element \n",
        "  option is FALSE, this field should be empty.\n",
        "}}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3fVYdnFOOR8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting your Google Drive\n",
        "The `mount()` function in Colab mounts your Google Drive account to the Colab runtime. This means that you can access all of the files in your Google Drive from within Colab, and you can also save files from Colab to your Google Drive.\n",
        "\n",
        "Please note this step will redirect to an authentication step, and it might take a few minutes if you have a lot of files in Drive.\n",
        "\n",
        "This step is optional. If you don't execute it, the files will be downloaded locally to the Colab instance."
      ],
      "metadata": {
        "id": "ArbJblMFP9ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "XiUi1h2nQEzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Imports"
      ],
      "metadata": {
        "id": "3JW2teMcP3vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import vertexai\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from googleapiclient import discovery\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import language_v2\n",
        "from google.colab import (\n",
        "    auth,\n",
        "    files\n",
        ")\n",
        "from vertexai.generative_models import (\n",
        "    GenerativeModel,\n",
        "    HarmCategory,\n",
        "    HarmBlockThreshold,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    GenerationConfig\n",
        ")"
      ],
      "metadata": {
        "id": "CwI0u_mH7qbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authentication\n",
        "\n",
        "The code below will create credentials to be used for Vertex and YT API.\n",
        "\n",
        "The authentication process gives rights to the following scopes:\n",
        "\n",
        "  \n",
        "*   cloud-platform.readonly\n",
        "*   youtube.readonly\n",
        "\n"
      ],
      "metadata": {
        "id": "yx1IZsSDQK06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCOPES = [\n",
        "    \"https://www.googleapis.com/auth/cloud-platform.read-only\",\n",
        "    \"https://www.googleapis.com/auth/youtube.readonly\",\n",
        "]\n",
        "\n",
        "service_account_upload = files.upload()\n",
        "service_account_json = json.loads(next(iter(service_account_upload.values())))\n",
        "credentials = service_account.Credentials.from_service_account_info(\n",
        "        service_account_json, scopes=SCOPES)"
      ],
      "metadata": {
        "id": "Az2tJc8KQM3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenAI Configuration\n",
        "\n",
        "This section configures the Vertex API. It uses the `gemini-2.5-pro-exp-03-25` to generate the video description and the credentials set in the previous step.\n",
        "\n",
        "It also sets the Gemini safety config to not block harmful content. This will allow to analyze every video, regardless of the content."
      ],
      "metadata": {
        "id": "B2DbfzZgQl1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vertexai.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)\n",
        "model = GenerativeModel(\"gemini-2.5-pro-exp-03-25\")\n",
        "\n",
        "safety_config = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_CIVIC_INTEGRITY,\n",
        "        threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "def get_gemini_analysis(prompt, video_url):\n",
        "  \"\"\"Analyzes a video using the Gemini model based on the provided prompt.\n",
        "\n",
        "  This function takes a text prompt and a video URL as input, sends them to the\n",
        "  Gemini model for analysis, and returns the structured JSON response. The\n",
        "  response is expected to conform to a predefined schema indicating the\n",
        "  presence of a specific element and providing optional details.\n",
        "\n",
        "  Args:\n",
        "      prompt: The text prompt to guide the Gemini model's analysis of the video.\n",
        "      video_url: The URL of the video file to be analyzed \n",
        "      (must be a supported format, e.g., \"video/mp4\").\n",
        "\n",
        "  Returns:\n",
        "      A dictionary representing the JSON response from the Gemini model. The\n",
        "      dictionary will have the following structure:\n",
        "      {\n",
        "          \"has_element\": \"TRUE\" or \"FALSE\",\n",
        "          \"details\": \"Optional details about the analysis\" or None\n",
        "      }\n",
        "\n",
        "  Raises:\n",
        "      Exception: If there is an error during the content generation process,\n",
        "                 the exception is caught, an error message is printed, and the\n",
        "                 exception is re-raised.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    response_schema = {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"has_element\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"enum\": [\"TRUE\", \"FALSE\"],\n",
        "                \"nullable\": False\n",
        "            },\n",
        "            \"details\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"nullable\": True\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"has_element\", \"details\"],\n",
        "    }\n",
        "\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=0.8,\n",
        "        max_output_tokens=8192,\n",
        "        response_schema=response_schema,\n",
        "        response_mime_type=\"application/json\",\n",
        "    )\n",
        "\n",
        "    video_part = Part.from_uri(video_url, mime_type=\"video/mp4\")\n",
        "\n",
        "    responses = model.generate_content(\n",
        "        [video_part, prompt],\n",
        "        generation_config=generation_config,\n",
        "        stream=False,\n",
        "        safety_settings=safety_config,\n",
        "    )\n",
        "    return json.loads(responses.text)\n",
        "  except Exception as e:\n",
        "    print(\"Error generating content: \" + str(e))\n",
        "    raise e"
      ],
      "metadata": {
        "id": "k6gly6A3QoQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Youtube API Configuration"
      ],
      "metadata": {
        "id": "7DPibxDFRTx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "YT_API_SERVICE_NAME = \"youtube\"\n",
        "YT_API_VERSION = \"v3\"\n",
        "\n",
        "youtube_api = discovery.build(YT_API_SERVICE_NAME, YT_API_VERSION, credentials=credentials)\n",
        "\n",
        "\n",
        "def get_viewcounts(video_ids, df):\n",
        "  \"\"\"Retrieves and updates the view counts for a list of YouTube videos.\n",
        "\n",
        "  This function uses the YouTube Data API to fetch the statistics (specifically\n",
        "  the view count) for a batch of video IDs. It then updates the 'views' column\n",
        "  in the provided Pandas DataFrame for the corresponding videos.\n",
        "\n",
        "  Args:\n",
        "      video_ids: A list of YouTube video IDs for which to retrieve view counts.\n",
        "      df: The Pandas DataFrame containing video information, including a\n",
        "          'video_id' column. The 'views' column will be updated in place.\n",
        "  \"\"\"\n",
        "  response_vids = youtube_api.videos().list(\n",
        "        part=\"statistics\",\n",
        "        id=\",\".join(video_ids)\n",
        "    ).execute()\n",
        "\n",
        "  for item in response_vids[\"items\"]:\n",
        "    if(\"viewCount\" not in item[\"statistics\"]):\n",
        "      df.loc[df.video_id == item[\"id\"], \"views\"] = 0\n",
        "    else:\n",
        "      df.loc[df.video_id == item[\"id\"], \"views\"] = item[\"statistics\"][\"viewCount\"]\n",
        "\n",
        "\n",
        "def get_videos_with_viewcounts_sorted(channel_id, video_count = None):\n",
        "  \"\"\"Retrieves and sorts a channel's recent videos, \n",
        "     limited by HARD_LIMIT variable, by view count\n",
        "\n",
        "  Fetches video metadata and view counts from the specified YouTube channel,\n",
        "  sorts them by views (descending), and optionally limits the number of results.\n",
        "\n",
        "  Args:\n",
        "      channel_id (str): The YouTube channel ID.\n",
        "      video_count (int, optional): Maximum number of videos to return. \n",
        "      Defaults to None (all).\n",
        "\n",
        "  Returns:\n",
        "      pandas.DataFrame: DataFrame of videos sorted by views, with columns:\n",
        "        'video_id', 'video_url', 'title', 'views', 'has_element', 'details'.\n",
        "  \"\"\"\n",
        "  next_token = \"\"\n",
        "  upload_playlist_id = \"UU\" + channel_id[2:]\n",
        "  video_ids = []\n",
        "\n",
        "  df = pd.DataFrame(columns=[\"video_id\", \"video_url\", \"title\", \n",
        "                             \"views\", \"has_element\",\"details\"])\n",
        "  df_counter = 0\n",
        "\n",
        "  while True:\n",
        "    reponse = youtube_api.playlistItems().list(\n",
        "        part=\"snippet\",\n",
        "        playlistId=upload_playlist_id,\n",
        "        maxResults=500,\n",
        "        pageToken=next_token\n",
        "    ).execute()\n",
        "\n",
        "    for item in reponse[\"items\"]:\n",
        "      videoId = item[\"snippet\"][\"resourceId\"][\"videoId\"]\n",
        "      title = item[\"snippet\"][\"title\"]\n",
        "\n",
        "      df.loc[df_counter] = [videoId, \"https://www.youtube.com/watch?v=\" \n",
        "                            + videoId, title, \"\", \"\", \"\"]\n",
        "      df_counter += 1\n",
        "      video_ids.append(videoId)\n",
        "\n",
        "      if df_counter \u003e= HARD_LIMIT:\n",
        "        break\n",
        "\n",
        "    get_viewcounts(video_ids, df)\n",
        "    video_ids.clear()\n",
        "    \n",
        "    if df_counter \u003e= HARD_LIMIT:\n",
        "      break\n",
        "\n",
        "    if \"nextPageToken\" in reponse:\n",
        "      next_token = reponse[\"nextPageToken\"]\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  df = df.sort_values(by=[\"views\"], ascending=False, key=pd.to_numeric)\n",
        "  return df.head(video_count) if video_count is not None else df"
      ],
      "metadata": {
        "id": "-uCefE-YWNwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Application\n",
        "\n",
        "This section will call each method to:\n",
        "\n",
        " * Get the list of the videos, sorted by views, from the configured channel.\n",
        " * Ask Gemini to look for the element configured and return the ocurrences in terms of timestamp.\n",
        " * If an error occurs, it will try again, up to a maximum of `max_retry` times."
      ],
      "metadata": {
        "id": "sfcOYr0Qmobn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retry_att = 1;\n",
        "max_retry = 3;\n",
        "\n",
        "df = get_videos_with_viewcounts_sorted(CHANNEL_ID, VIDEO_COUNT)\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  while(retry_att \u003c= max_retry):\n",
        "    print(f'Reading https://youtu.be/{row[\"video_id\"]} at attempt #{retry_att}')\n",
        "    try:\n",
        "      analysis = get_gemini_analysis(FINAL_PROMPT, row[\"video_url\"])\n",
        "      print(analysis)\n",
        "      df.at[index, \"has_element\"] = analysis[\"has_element\"]\n",
        "      df.at[index, \"details\"] = analysis[\"details\"]\n",
        "      break\n",
        "    except Exception as e:\n",
        "      if retry_att + 1 \u003c= max_retry:\n",
        "        print(f'Error #{retry_att}, retrying...')\n",
        "        retry_att += 1\n",
        "        time.sleep(5)\n",
        "        continue\n",
        "      print(f'Max number of retries has been reached; moving on... [{str(e)}]', )\n",
        "      df.at[index, \"has_element\"] = \"ERROR\"\n",
        "      df.at[index, \"details\"] = str(e)\n",
        "      break\n",
        "  retry_att = 1"
      ],
      "metadata": {
        "id": "pALAs8YEM3Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persist into GDrive\n",
        "\n",
        "After the script runs, this code will save the results to a folder in Google Drive, as configured by the user."
      ],
      "metadata": {
        "id": "RiWr6VclYKIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = \"/content/drive/MyDrive/\"\n",
        "OUTPUT_FILE_NAME = f\"{CSV_NAME}.csv\"\n",
        "output_folder_path = os.path.join(gdrive_path, ROOT_FOLDER_NAME)\n",
        "output_file_path = os.path.join(output_folder_path, OUTPUT_FILE_NAME)\n",
        "\n",
        "os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "df.to_csv(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "-15eo6MbYPdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RnATilGPOoSl"
      },
      "cell_type": "markdown",
      "source": [
        "## Disconnect Google Drive from this Colab"
      ]
    },
    {
      "metadata": {
        "id": "e9qP-qKHOoSl"
      },
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()\n",
        "print(\"All changes made in this colab session should now be visible in Drive.\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}
