{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqWIENumgw-E"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VDffdwR_Qqf"
      },
      "source": [
        "In this Colab, we intend to create YouTube Shorts from a long-form VOD video on YouTube using YouTube Analytics and Artificial Intelligence on the video !\n",
        "\n",
        "We will use the  [YouTube Analytics API](https://developers.google.com/youtube/analytics) to find the most engaging segments of the video and then on each segment of the video that is most likely to perform, we will create a vertically framed version of it using [Video Intelligence API](https://cloud.google.com/video-intelligence/docs/), the Google Cloud Platform service for AI on Videos.\n",
        "\n",
        "So just read the text cells, provide your inputs, execute all the code cells and get ready to see some magic Shorts.\n",
        "\n",
        "**Prerequisite**: In order to execute this colab you need to have a CMS. A Content Manager System is a web-based tool for partners who manage content and rights on YouTube. A Content Manager account owns one or more YouTube channels and the assets associated with them. Also known as Studio Content Manager. Here is the [full documentation](https://support.google.com/youtube/answer/6301172?hl=en\u0026sjid=16179625971008095581-EU). You can go to your CMS by going to [the studio page](https://studio.youtube.com/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUJiVdxwgy-J"
      },
      "source": [
        "# Shorts proposal from Long form video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB8QBV28HbB2"
      },
      "source": [
        "\n",
        "\n",
        "First, let's complete some preparation steps to set up your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibUsbBxUTfZS"
      },
      "source": [
        "## 0. Preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIpegk10Ja8G"
      },
      "source": [
        "### a. Install Relevant Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QTaHgE9Hgc3"
      },
      "source": [
        "We will use a few packages for this Colab: MoviePy for video editing and the Video Intelligence API for object detection on frames. When executing the installation command lines, you might need to restart the session. If so, you can skip the installation step once the session has restarted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m79H588YBiH"
      },
      "outputs": [],
      "source": [
        "# useful install\n",
        "!pip install moviepy\n",
        "!pip install --upgrade google-cloud-videointelligence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH1cDf06JimX"
      },
      "source": [
        "###b. Set up all needed elements for the Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg9c8OH3JnXU"
      },
      "source": [
        "\n",
        " **Step 0: Get Video URL and mp4 file**\n",
        "\n",
        "\n",
        "\n",
        "* Get the video URL you want to analyse. For example: https://www.youtube.com/watch?v=a1b2c3d4\n",
        "\n",
        "\n",
        "\n",
        "**Step 1: Create a Google Cloud Platform Project**\n",
        "\n",
        "* Create a Google Cloud Platform project if don't already have one. Follow instructions [here](https://developers.google.com/workspace/guides/create-project) on how to set that up.\n",
        "\n",
        "**Step 2: Enable APIs**\n",
        "\n",
        "* Enable the YouTube Analytics API [here](https://console.cloud.google.com/marketplace/product/google/youtubeanalytics.googleapis.com)\n",
        "\n",
        "* Enable the Video Intelligence API [here](https://console.cloud.google.com/marketplace/product/google/videointelligence.googleapis.com)\n",
        "\n",
        "**Step 3: Create a Google Cloud Platform bucket**\n",
        "\n",
        "You need a Google Cloud Platform Storage bucket for your project. Follow the tutorial [here](https://cloud.google.com/storage/docs/creating-buckets) to create Google Cloud Platform buckets.\n",
        "Once this is done you can create 3 folders within your Google Cloud Platform bucket:\n",
        "- one called `input`: this one will contain the video(s) you want to process\n",
        "- one called `temp`: this one will contain all your temporary files.\n",
        "- one called `output`': this one will contain all the created Shorts from the longform video.\n",
        "\n",
        "**Step 4: Create a Service account for authentication to the YouTube Analytics API**\n",
        "\n",
        "* Create a service account in the IAM section and a service account key for it using the [following tutorial](https://cloud.google.com/iam/docs/keys-create-delete).\n",
        "* Get the email from the service account key. It ends with iam.gserviceaccount.com and can be found on your credentials page clicking on the key you created in the Details \u003e e-mail section.\n",
        "\n",
        "* Then generate a JSON key file for this service account using the tutorial [here](https://developers.google.com/workspace/guides/create-credentials#create_credentials_for_a_service_account) (Section: Create Credentials for service account)\n",
        "\n",
        "* Add your service account email as an admin in your CMS using the [following tutorial](https://support.google.com/youtube/answer/4524878).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dsQRhTMZ0lt"
      },
      "source": [
        "### c. Set all necessary variables\n",
        "\n",
        "Please enter below the different parameters:\n",
        "\n",
        "\n",
        "1. Your Google Cloud Platform Project ID. This can be found on the home page of the [Google Cloud Console](https://console.cloud.google.com/).\n",
        "\n",
        "2. Your Google Cloud Bucket name: this is the name of the bucket created in step 3 above (which contains your 3 folders: 'input', 'output', 'temp').\n",
        "\n",
        "3. The video id: this is what comes after `watch?v=` in your video URL. Example: for the video https://www.youtube.com/watch?v=XXXXXXXX it will be XXXXXXXX.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ch7s_5QMHbR"
      },
      "outputs": [],
      "source": [
        "\n",
        "global GOOGLE_CLOUD_PROJECT_ID\n",
        "global CONTENT_OWNER_ID\n",
        "global BUCKET_NAME\n",
        "global VIDEO_ID\n",
        "\n",
        "# Google Cloud Project ID\n",
        "\n",
        "GOOGLE_CLOUD_PROJECT_ID = ''  #@param {type:\"string\"}\n",
        "\n",
        "# Google Cloud Bucket name\n",
        "BUCKET_NAME = ''  #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Content owner ID\n",
        "CONTENT_OWNER_ID = ''  #@param {type:\"string\"}\n",
        "\n",
        "# Video ID\n",
        "VIDEO_ID = '' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bDpLl3BJ-wD"
      },
      "source": [
        "### d. Import all relevant python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L_hnvZwHkMG"
      },
      "outputs": [],
      "source": [
        "# Useful Imports\n",
        "# Standard packages\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import date, timedelta\n",
        "import io\n",
        "#Google clients\n",
        "\n",
        "from googleapiclient import discovery\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import drive\n",
        "import google.api_core.exceptions as gcs_exceptions\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "from google.cloud import videointelligence\n",
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "# Installed packages\n",
        "import moviepy.editor as mov\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "758DSCo8HoqA"
      },
      "source": [
        "# 1. Get the best performing segments of the video using YouTube Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8nIHRysZJe6"
      },
      "source": [
        "In this section, we are going to create Shorts version of longform videos using the Analytics API relativeRetentionPerformance metric.\n",
        "\n",
        "The [YouTube Analytics API](https://developers.google.com/youtube/analytics) is the API that allows you to get metrics on your videos when you are authenticated. You can get metrics like watch time, views etc.\n",
        "\n",
        "To explain further, the [relative Retention Performance metric](https://developers.google.com/youtube/analytics/metrics#Audience_Retention_Metrics) attributes a value between 0 and 1 for all segments of a given video (the video is split in 100 equal time segments).\n",
        "\n",
        "Those values correspond to how well the video performs compared to videos of the same length. So basically if the value is higher than 0.5, its already quite good. But what if all segments of your video have a value higher than 0.5?\n",
        "\n",
        "That's great, but you do not want to make 10 shorts out of your video, right? This why we will get the segments where RelativeRetentionPerformance is the highest within each video, which are considered **the most engaging segments** from your video.\n",
        "\n",
        "In this section we will:\n",
        "\n",
        "a. Upload your video to your Google Cloud bucket (created above) as an mp4 file.\n",
        "\n",
        "b. Authenticate to the YT Analytics API using your service account and get the RelativeRetentionPerformance metric for your video.\n",
        "\n",
        "c. Get the best performing segments of your video.\n",
        "\n",
        "d. Create the resulting video clips we will use to create YouTube Shorts videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYVZ7J-fHsQs"
      },
      "source": [
        "##a. Upload the longform video to your bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG1b_e4ma6gB"
      },
      "source": [
        "In your YouTube Studio, download the mp4 version of your video using the [following tutorial](https://support.google.com/youtube/answer/56100).\n",
        "\n",
        "Then go back to your Google Cloud Platform project, open the 'input' folder that you created in your  Google Cloud Platform bucket and upload the video you just downloaded in the bucket. Copy the name of your file and paste it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4u9wcEUbz-0"
      },
      "outputs": [],
      "source": [
        "global VIDEO_FILE\n",
        "VIDEO_FILE = \"\"  #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-mEHqiOZZrr"
      },
      "source": [
        "##b. Authentication to the YouTube Analytics API and get the RelativeRetentionPerformance data\n",
        "\n",
        "The code below will allow you to authenticate to the YouTube Analytics API and query the relativeRetentionPerformance data for your video.\n",
        "\n",
        "You will need to have completed the preparation step 4 of the previous section. This will allow to authenticate within the Colab environment as your service account which has rights on the CMS (granted in the previous steps) and which has rights to make calls on the API\n",
        "\n",
        "The authentication process gives rights to the following scope:\n",
        "- yt-analytics-monetary.readonly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv-sXxy187Io"
      },
      "source": [
        "When you run the next cell it will ask you for the json key you created and downloaded locally in the preparation step. Upload the file located locally to authenticate to the API with this service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baHMQbBUNMox"
      },
      "outputs": [],
      "source": [
        "\"\"\"YouTube Analytics API Authentication\"\"\"\n",
        "\n",
        "SCOPES = [\n",
        "          'https://www.googleapis.com/auth/yt-analytics.readonly'\n",
        "          ]\n",
        "\n",
        "\n",
        "API_SERVICE_NAME = 'youtubeAnalytics'\n",
        "API_VERSION = 'v2'\n",
        "\n",
        "\n",
        "# Authorize the request and store authorization credentials.\n",
        "def get_authenticated_service():\n",
        "  service_account_upload = files.upload()\n",
        "  service_account_js = json.loads(\n",
        "      next(iter(service_account_upload.values()))\n",
        "  )\n",
        "  credentials = service_account.Credentials.from_service_account_info(\n",
        "        service_account_js, scopes=SCOPES)\n",
        "  print('Success! You are now authenticated to the YouTube Analytics API')\n",
        "  return build(API_SERVICE_NAME, API_VERSION, credentials = credentials)\n",
        "\n",
        "\n",
        "youtube_analytics = get_authenticated_service()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL7MXKVvvxPP"
      },
      "source": [
        "Now, let's get the relativeRetentionPerformance data for the video. This can be obtained using the query getting the metric based on elapsedTimeRatio on specific dates.\n",
        "\n",
        "The relativeRetentionPerformance is calculated on a video for a definite set of time taking the performance of the video over this time frame.We prefer to consider all time performance here in order to avoid any seasonal effect or time-based bias, so we will take the largest time frame possible for each video.\n",
        "\n",
        "As the freshness of the data is 48 hours ([documentation](https://developers.google.com/youtube/reporting/v1/reports)), we will choose the startDate as the first date a [video](https://www.youtube.com/watch?v=jNQXAC9IVRw) was ever published to YouTube  and the endDate as 2 days ago to have the latest performance.\n",
        "\n",
        "The elapsedTimeRatio dimension breaks down the results, showing how well different percentages of your video retain viewers (for example, the first 1% of the video, the second 2%, and so on).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGwpnwlNNUZN"
      },
      "outputs": [],
      "source": [
        "# Calculate the D-2 Date which is the latest the Analytics will provide\n",
        "two_days_ago = date.today() - timedelta(days=2)\n",
        "\n",
        "# Format the date as a string in the 'YYYY-mm-dd' format\n",
        "formatted_date = two_days_ago.strftime('%Y-%m-%d')\n",
        "\n",
        "def execute_api_request(client_library_function, **kwargs):\n",
        "  response = client_library_function(\n",
        "    **kwargs\n",
        "  ).execute()\n",
        "  return response\n",
        "\n",
        "\n",
        "\n",
        "relative_retention_performance = execute_api_request(\n",
        "      youtube_analytics.reports().query,\n",
        "      filters=f'video=={VIDEO_ID}',\n",
        "      dimensions='elapsedVideoTimeRatio',\n",
        "      ids=f'contentOwner=={CONTENT_OWNER_ID}',\n",
        "      startDate='2005-04-23',\n",
        "      endDate= formatted_date,\n",
        "      metrics='relativeRetentionPerformance'\n",
        "  )\n",
        "print('Gathered the relativeRetentionMetrics for this video')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_P91B_idnDT"
      },
      "source": [
        "## c. Get the best segments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPIL-pw_b6Zw"
      },
      "source": [
        "The relative retention performance shows the performance of each segment.\\\n",
        "The output from the query above will be a list that will look like: \\\n",
        "\n",
        "`[[1st segment of the video, 0.75],[2nd segment of the video, 0.62]....,[100th segment of the video, 0.81]]`\n",
        "\n",
        "We'll now isolate the top 5% performing segments of your video to create engaging Shorts. Since this data isn't grouped together, we'll use a bit of code to organize the segments.\n",
        "\n",
        "This output will look like:\n",
        "\n",
        "`[[start_time_short_1, end_time_short_1],[start_time_short_2, end_time_short_2]...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOCbVX-9N5Ve"
      },
      "outputs": [],
      "source": [
        "def top_5_pc_short_segments(relative_retention_performance):\n",
        "  \"\"\"Gets the 5% top segments for a given video\n",
        "     Args::\n",
        "            - relativeRetentionPerformance\n",
        "            result of the Analytics query\n",
        "    [[xth segment of the video,\n",
        "    RelativeRetentionPerformance on the segment]...]\n",
        "\n",
        "    Output: \"Segments with a start and end time\n",
        "    [[start_time, end_time]...]\"\"\"\n",
        "\n",
        "  perfs = [l[1] for l in relative_retention_performance['rows']]\n",
        "\n",
        "  # Sort the floats in descending order\n",
        "  perfs.sort(reverse=True)\n",
        "\n",
        "  # Calculate the index of the top 5%\n",
        "  top_5_percent_index = int(0.05 * len(perfs))\n",
        "  limit = perfs[top_5_percent_index]\n",
        "  best_segments_short = [l for l in relative_retention_performance['rows']\n",
        "                          if l[1] \u003e= limit]\n",
        "  #  # Create segments with start and end times\n",
        "  segments = [[round(i[0]-0.01,2),\n",
        "               round(i[0],2)] for i in best_segments_short]\n",
        "  aggregated_segments = []\n",
        "\n",
        "  if segments:\n",
        "      current_segment = segments[0]\n",
        "      for segment in segments[1:]:\n",
        "          # Check adjacency with rounding\n",
        "          if current_segment[1] == segment[0]:\n",
        "              # Extend the current segment\n",
        "              current_segment[1] = segment[1]\n",
        "          else:\n",
        "              # Append the completed segment\n",
        "              aggregated_segments.append(current_segment)\n",
        "              # Start a new segment\n",
        "              current_segment = segment\n",
        "\n",
        "      # Append the final segment\n",
        "      aggregated_segments.append(current_segment)\n",
        "\n",
        "  return limit, aggregated_segments\n",
        "\n",
        "limit, best_segments = top_5_pc_short_segments(\n",
        "    relative_retention_performance\n",
        "    )\n",
        "print(f'Best performing segments: {best_segments}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gk0Q2AoIy3J"
      },
      "source": [
        "## d. Create Video clips from those segments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcDDpwwdH0nv"
      },
      "source": [
        "The section below will create video clips from the segments above. They will all be marked as temporary files in your 'temp' folder of your Google Cloud Platform bucket.\n",
        "\n",
        "**About the duration of your clips**\n",
        "\n",
        "From the section above we got the best segments but what if those add up to just a few seconds ?\n",
        "In order to prevent that we propose having a variable of min short duration, to set your minimum desired duration for your Short. We will manually add some padding time to make sure your segment reaches the minimum duration.\n",
        "The segments will also be trimmed if they are longer than 1mn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qf0i28LY0XL"
      },
      "outputs": [],
      "source": [
        "global MIN_SHORT_DURATION_S\n",
        "MIN_SHORT_DURATION_S = 0  #@param {type:\"number\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg7AXuqufSjK"
      },
      "source": [
        "First we will start by reading the video from your file location URI on Google Cloud Storage\n",
        "\n",
        "The cell below will prompt you to authenticate your project. It will ask you to click on a link below, which opens in a new tab. This will prompt you to choose a Google account to authenticate with. After choosing the one which has access to your Google Cloud Platform project then you'll need to copy the code on the page and paste it below where it says \"Enter Authorization Code\".\n",
        "\n",
        "The authentication process gives rights to the following scopes:\n",
        "\n",
        "* cloud-platform.readonly\n",
        "\n",
        "\n",
        "Technically, scopes define certain access permissions within your Google Cloud Platform project, and it virtually creates credentials in a JSON file, which are used to authenticate to the API services.\n",
        "\n",
        "In order to execute the following cells you will need storage.objects.create access to Google Cloud Storage. You can add yourself as storage object creator role in the bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXuy6SLOfJh4"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT_ID\n",
        "!gcloud auth application-default login \\\n",
        "    --scopes='https://www.googleapis.com/auth/cloud-platform'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9PkoEuj-A4n"
      },
      "source": [
        "The cells below are utils to download Cloud Storage files locally and upload local files to a Google Cloud Platform bucket.  We will need 2 functions here to download and upload elements from your bucket on Google Cloud Platform. The full documentation for object comprehension on Storage can be found [here](https://cloud.google.com/storage/docs/objects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFP8h2MfkG7F"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "\n",
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a blob from the bucket.\n",
        "    A blob in GCP is the ID of a GCS object\"\"\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    try:\n",
        "      blob.download_to_filename(destination_file_name)\n",
        "      print(\n",
        "          \"Downloaded storage object {} \"\n",
        "          \"from bucket {} to local file {}.\"\n",
        "          .format(\n",
        "              source_blob_name,\n",
        "              bucket_name,\n",
        "              destination_file_name\n",
        "          )\n",
        "      )\n",
        "    except Exception as e:  # Catch any error\n",
        "      print(f\"An error occurred during upload: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def upload_blob(bucket_name,\n",
        "                source_file_name,\n",
        "                destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    gen_match_precondition = 0\n",
        "    try:\n",
        "      blob.upload_from_filename(\n",
        "          source_file_name,\n",
        "          if_generation_match=gen_match_precondition)\n",
        "      print(f\"File {source_file_name} \"\n",
        "            f\"uploaded to {destination_blob_name}.\")\n",
        "    # Catch 412 error\n",
        "    except gcs_exceptions.PreconditionFailed:\n",
        "      print(f\"File {destination_blob_name} already exists in the bucket.\")\n",
        "\n",
        "    except Exception as e:  # Catch any other potential errors\n",
        "      print(f\"An error occurred during upload: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PzPAo5oclL4"
      },
      "source": [
        "The code section below creates video clips from the segments calculated in part b. We will use the MoviePy python package to create the videos.\n",
        "As MoviePy requires local file access, we must first download the video from the bucket, process it locally, create the clips, and then upload them to the Google Cloud Platform bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cXovi_POT2i"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_video_clips(bucket, video_file, segments, min_short_duration):\n",
        "    \"\"\"\n",
        "    Creates video clips based on the\n",
        "    provided segments from the input video.\n",
        "\n",
        "    Args:\n",
        "        video_filename (str): Name of the input video file\n",
        "                            (including path\n",
        "                            if not present in the same folder).\n",
        "        segments (list): A list of segments in the format\n",
        "                        [[start_percent, end_percent], ...].\n",
        "        output_folder (str, optional): Name of the output folder.\n",
        "                                       Defaults to \"clips\".\n",
        "\n",
        "    \"\"\"\n",
        "    # download locally the file from the bucket\n",
        "    download_blob(bucket, f'input/{video_file}', video_file)\n",
        "    video = mov.VideoFileClip(video_file)\n",
        "    video_duration = video.duration\n",
        "    for i, segment in enumerate(segments):\n",
        "        start_time = segment[0] * video_duration\n",
        "        end_time = segment[1] * video_duration\n",
        "        duration = end_time - start_time\n",
        "        # Clip is shorter than 15 seconds\n",
        "        if duration \u003c min_short_duration :\n",
        "          padding_amount = min_short_duration - duration\n",
        "          if start_time - padding_amount/2 \u003c 0:\n",
        "            # then we take the first min_duration_s as the short\n",
        "            start_time = 0\n",
        "            end_time = min_short_duration\n",
        "          elif end_time + padding_amount/2 \u003e  video_duration :\n",
        "            # then we take the last min_duration_s as the short\n",
        "            start_time = video_duration - min_short_duration\n",
        "            end_time = video_duration\n",
        "          else:\n",
        "            #otherwise we can pad equally at start and end\n",
        "            start_time = start_time - padding_amount/2\n",
        "            end_time = end_time + padding_amount/2\n",
        "\n",
        "        if duration\u003e60:\n",
        "          excess_duration = duration - 60\n",
        "          # Trim equally from start and end\n",
        "          trim_amount = excess_duration / 2\n",
        "\n",
        "          start_time = start_time + trim_amount\n",
        "          end_time = end_time - trim_amount\n",
        "        clip = video.subclip(start_time, end_time)\n",
        "        base_name =  re.sub(r\"\\.[^.]*$\", \"\", video_file)\n",
        "        output_file_local = (\n",
        "                f'temporary_{base_name}_top_segment_{i+1}.mp4'\n",
        "        )\n",
        "        clip.write_videofile(output_file_local)\n",
        "\n",
        "        # upload the file created to bucket\n",
        "\n",
        "        output_blob = (\n",
        "            f\"temp/temporary_{base_name}_top_segment_{i+1}.mp4\"\n",
        "        )\n",
        "        upload_blob(bucket, output_file_local, output_blob)\n",
        "\n",
        "\n",
        "    video.close()\n",
        "    print('All done!')\n",
        "\n",
        "# Creation of video Clips\n",
        "create_video_clips(bucket = BUCKET_NAME,\n",
        "                  video_file = VIDEO_FILE,\n",
        "                  segments = best_segments,\n",
        "                  min_short_duration = MIN_SHORT_DURATION_S)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjCRU2cVH7Kt"
      },
      "source": [
        "The result produced here is a collection of videos which represents the best segments of the input video in VOD format in a temp folder of your Google Cloud Platform bucket. The next part will be focused on creating the vertically cropped version of those videos  to make Shorts formats of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ajzCHUTXzn"
      },
      "source": [
        "# 2. Image Analysis with Video Intelligence API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sInUr3Encdo0"
      },
      "source": [
        "The section below will propose a vertically cropped version of the video segment from the video segment mp4 file(s) created above.\n",
        "\n",
        "We will use for that Google Video Intelligence API from  Google Cloud Platform. You can check the full documentation of Video Intelligence API [here](https://cloud.google.com/video-intelligence/docs/).\n",
        "\n",
        "\n",
        "\n",
        "This API is a paid service on Google Cloud Platform. You can find all billing information [here](https://cloud.google.com/video-intelligence/pricing).\n",
        "\n",
        "Otherwise if you want a solution that is free of charge, you can use [Mediapipe](https://research.google/pubs/mediapipe-a-framework-for-perceiving-and-processing-reality/), a framework released by Google Research that allows to make inference (full paper [here](https://static1.squarespace.com/static/5c3f69e1cc8fedbc039ea739/t/5e130ff310a69061a71cbd7c/1578307584840/NewTitle_May1_MediaPipe_CVPR_CV4ARVR_Workshop_2019.pdf))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soMXBU4nW2oZ"
      },
      "source": [
        "## a. Selection of the segments to crop vertically\n",
        "\n",
        " Check on your bucket the segments created and enter below the segment for which you want to pursue the study. All the segments created end with `_top_segment_{SEGMENT_NUMBER}.mp4` Choose the segment you want to treat from the list of segments created and input the `SEGMENT_NUMBER` in the cell below.\n",
        "\n",
        "\n",
        "For example : The first segment will be a file ending with _top_segment_1.mp4 in your temp bucket. If you want to treat _top_segment_1 enter `SEGMENT_NUMBER = 1`.\n",
        "\n",
        "\n",
        "If you want to produce the resulting vertically cropped Shorts video, re-execute the section below changing the segment number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KVHyowsWoqi"
      },
      "outputs": [],
      "source": [
        "global SEGMENT_NUMBER\n",
        "SEGMENT_NUMBER = 1 #@param {type:\"number\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZKxuPR-XVD1"
      },
      "source": [
        "The code section below allows to have the video as an input file object in order to execute the video intelligence API Object Tracking method on it and visualize the detected objects using Visualizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn8p62GEWhHw"
      },
      "source": [
        "## b. Detect objects within the video and visualize it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtL2dV0IV1eh"
      },
      "source": [
        "\n",
        "The Video Intelligence API allows developers to use Google video analysis technology as part of their applications. The REST API enables users to annotate videos stored locally or in Cloud Storage, or live-streamed, with contextual information at the level of the entire video, per segment, per shot, and per frame.\n",
        "\n",
        "We will focus on one service here which is Object Tracking (documentation [here](https://cloud.google.com/video-intelligence/docs/feature-object-tracking) ). Object Tracking allows you to see in a video the objects that appear and for each object all the frames for the video on which they appear, their timestamp on the video and the bounding boxes.\n",
        "We will provide a complementary section below based on your result of object tracking a quick tutorial to visualize those bounding boxes on the [Visualizer](https://zackakil.github.io/video-intelligence-api-visualiser/#Object%20Tracking), an OpenSource tool that allows you to visualize the results from the VideoIntelligenceAPI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaXtn-I4X6hW"
      },
      "source": [
        "The response object is the result of the Video Intelligence API. This result is going to contain all the objects found with all the frames from the video (24 frames per second) where they appear with the bounding boxes. Sounds like a lot of information right ?\n",
        "In order to make it more visualizable, this step  is just to help you visualize the objects that are detected on the Visualizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAv6Y7WV0q8o"
      },
      "outputs": [],
      "source": [
        "video_client = videointelligence.VideoIntelligenceServiceClient()\n",
        "features = [videointelligence.Feature.OBJECT_TRACKING]\n",
        "base_name =  re.sub(r\"\\.[^.]*$\", \"\",VIDEO_FILE)\n",
        "segment = f\"temporary_{base_name}_top_segment_\" \\\n",
        "          f\"{SEGMENT_NUMBER}.mp4\"\n",
        "segment_blob = 'temp/' + segment\n",
        "segment_detections_blob = f\"temp/\" \\\n",
        "                          f\"temporary_{base_name}_top_segment_\" \\\n",
        "                          f\"{SEGMENT_NUMBER}.json\"\n",
        "operation = video_client.annotate_video(\n",
        "        request={\n",
        "            \"features\": features,\n",
        "            \"input_uri\": f'gs://{BUCKET_NAME}/{segment_blob}',\n",
        "            \"output_uri\": f'gs://{BUCKET_NAME}/{segment_detections_blob}'}\n",
        "    )\n",
        "print(f'Your video clip is located at :'\n",
        "      +f'gs://{BUCKET_NAME}/{segment_blob}')\n",
        "print(f'Your file containing all the objects detect is located at : '\n",
        "      f'gs://{BUCKET_NAME}/temp/{segment_detections_blob}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPlgALzRYckH"
      },
      "source": [
        "\n",
        "Now you can download the 2 files in the location printed above in your computer and go to the [visualizer](https://zackakil.github.io/video-intelligence-api-visualiser/#Object%20Tracking)\n",
        "\n",
        "The first file is your video clip (.mp4) and the second one is a JSON file (.json) containing all objects detected by the Video Intelligence API Object Tracker.\n",
        "\n",
        "Upload the segment detections blob json file in 'your .json' section and your video in the 'your .mp4'\n",
        "\n",
        "This visualizer helps you see all the objects no matter the confidence level. Play with the confidence threshold to see the most probable objects and select a confidence for which the objects found make sense for you.\n",
        "Usually 0.8 is a good threshold to make sure we are not considering objects that aren't there, but it might depend on your case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnbIcSXhzmbV"
      },
      "source": [
        "## d. Create the focus points for each frame using the ObjectTracking results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a4VcjwBcmCM"
      },
      "source": [
        "The following part  function gets for each frame a focus point that will be the horizontal center of the frame.\n",
        "\n",
        "To achieve frame-by-frame comprehension, we need to restructure the VideoIntelligenceAPI object result. Currently, the VideoIntelligenceAPI output is a list of objects, where each object contains the frames (timestamps) and bounding boxes for object appearances. Instead, we want the center of the biggest object for each video frame (timestamp). This requires reversing the comprehension logic to extract the coordinates for each frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slb97SoQlK2D"
      },
      "outputs": [],
      "source": [
        "global CONFIDENCE_THRESHOLD\n",
        "CONFIDENCE_THRESHOLD = 0.8 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAG23wJ-Tua9"
      },
      "outputs": [],
      "source": [
        "def get_frames_from_bbox(object_annotations,\n",
        "                         width,\n",
        "                         height,\n",
        "                         nb_frames,\n",
        "                         fps,\n",
        "                         confidence_thr):\n",
        "  \"\"\"\n",
        "  Creates frames and focus points based on the annotations\n",
        "  from the ObjectTracking results\n",
        "\n",
        "  Args:\n",
        "      object_annotations (dict)): Annotations as a dictionnary\n",
        "\n",
        "      width (int): the width of the video\n",
        "\n",
        "      height (int): the height of the video\n",
        "\n",
        "      nb_frames: the number of frames in the video\n",
        "\n",
        "      confidence_threshold (float): the threshold of objects to keep\n",
        "                                    in the output\n",
        "  Output:\n",
        "\n",
        "      all_frames: a list of lists that contains for every frame\n",
        "                  all the objects that are detected\n",
        "                  with a confidence higher than the threshold\n",
        "\n",
        "      focus_points: a list that contains for each frame\n",
        "                    the focus point being the center\n",
        "                    of the biggest bounding box detected in the frame\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  all_frames = [[] for i in range(nb_frames)]\n",
        "  for object_annotation in object_annotations:\n",
        "    confidence = object_annotation.confidence\n",
        "    if confidence \u003e confidence_thr:\n",
        "      object_name = object_annotation.entity.description\n",
        "      for frame in object_annotation.frames:\n",
        "        offset = frame.time_offset\n",
        "        time_frame_s = offset.seconds + offset.microseconds/1e6\n",
        "        time_frame_abs = int(time_frame_s*fps)\n",
        "        right = frame.normalized_bounding_box.right\n",
        "        left = frame.normalized_bounding_box.left\n",
        "        top = frame.normalized_bounding_box.top\n",
        "        bottom = frame.normalized_bounding_box.bottom\n",
        "        appended_object = {'object_name': object_name,\n",
        "            'confidence': confidence,\n",
        "            'bbox_left': left*width,\n",
        "            'bbox_right': right*width,\n",
        "            'bbox_top': top*height,\n",
        "            'bbox_bottom': bottom*height,\n",
        "            'surface': (right-left)*(bottom - top)*width*height\n",
        "            }\n",
        "        all_frames[time_frame_abs].append(appended_object)\n",
        "\n",
        "\n",
        "  focus_points = []\n",
        "  # go back on the frame dictionnary and select the biggest bounding box\n",
        "  prev_focus_x = None\n",
        "\n",
        "  for frame_objects in all_frames :\n",
        "    if frame_objects:\n",
        "      biggest_object = max(frame_objects,\n",
        "                           key=lambda obj: obj[\"surface\"])\n",
        "      focus_x = (biggest_object['bbox_right']\n",
        "                + biggest_object['bbox_left'])/2\n",
        "      focus_points.append(focus_x)\n",
        "      prev_focus_x = focus_x\n",
        "    else:\n",
        "      focus_points.append(prev_focus_x)\n",
        "  return all_frames, focus_points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nxZtbhbEo3b"
      },
      "source": [
        "The cell below will produce all the focus points by timeframe.\n",
        "\n",
        "\n",
        "First here we download the segment from the  Google Cloud Platform bucket to read it locally. \\\n",
        "\n",
        "Then, we execute the functions above on this segment video so we get the object annotations, the frames and the focus points from this video.\n",
        "\n",
        "As the focus points should represent an object you will follow, the focus points should evolve smoothly. However since the bounding box can be a bit unstable you will see below in the resulting plot that usually the focus points are evolving with some glitches. If you create a video from those, the video itself will glitch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiyPD_uaUJWk"
      },
      "outputs": [],
      "source": [
        "# Load the video: Download the segment as a blob\n",
        "# from GCP and read it locally\n",
        "\n",
        "download_blob(BUCKET_NAME,\n",
        "              segment_blob,\n",
        "              segment)\n",
        "\n",
        "cap = cv2.VideoCapture(segment)\n",
        "\n",
        "# Get video properties for output video setup\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "nb_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "duration_s = nb_frames/fps\n",
        "# Get the annotations\n",
        "\n",
        "response = operation.result()\n",
        "object_annotations = response.annotation_results[0].object_annotations\n",
        "\n",
        "all_frames, focus_points = get_frames_from_bbox(\n",
        "                              object_annotations,\n",
        "                              width,\n",
        "                              height,\n",
        "                              nb_frames,\n",
        "                              fps,\n",
        "                              CONFIDENCE_THRESHOLD\n",
        "                              )\n",
        "\n",
        "#Print the focus points\n",
        "pd.Series(focus_points).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYdY2j5B03WM"
      },
      "source": [
        "The bounding box usually look smooth on the visualizer but from the plotted version of the focus points, you will agree that those focus points don't look super smooth right ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqQ8S9xYzyBK"
      },
      "source": [
        "## e. Smooth the Focus points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEBVyM3TdszO"
      },
      "source": [
        "The evolution above of the focus points is usually not smooth. Therefore, if we take each center of frame and aggregate cropped images from that, we will have a resulting glitching video, in the sense that the centers might be slightly offset. In order to avoid that we will propose 2 smoothing techniques to avoid glitches\n",
        "\n",
        "Both techniques use rolling windows to compute a smoothed version of x at each frame.\n",
        "\n",
        "- **The first method** will generate centers that are **piecewise constant**. What this means is that we are going to take on rolling windows the x coordinate that appears the most. This will work best if your focus point is quite fix on the image.  It will be adapted **if you know your camera is static**.\n",
        "- **The second method** will generated centers that are **moving linearly**. What this means is that for each window we take the median of the values of x. This method is more adapted **if your video has a moving subjects**\n",
        "\n",
        "\n",
        "The window size can also be set\n",
        "\n",
        "- **small value for window size** \u003c24 : This will mean that we will search for the median or static value over 24 frames. This will produce focus points that will be closer to the the actual values. It is most adapted if you see that the focus points have a linear evolution for instance with few glitches.\n",
        "- **high value for window size** \u003e48: This will mean that we will search for the median or static value over 24 frames. This will produce focus points that will be a lot more static looking. This is most adapted if you know that you have a lot of glitches (a lot of small bumps in the previous graph).\n",
        "\n",
        "We set the input to 48 which usually produces nice results but if you see that the frames don't move quick enough, you can reduce the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvX-XbI7XNPZ"
      },
      "outputs": [],
      "source": [
        "def compute_new_focuses(x,method, window_size):\n",
        "  \"\"\" Creates the new focus points smoothed\n",
        "  according to a method and window size\n",
        "  Args:\n",
        "    x (list) : focus points\n",
        "    method (str) : 'static' or 'median'\n",
        "    window_size (int): size of the rolling window\n",
        "\n",
        "  Output:\n",
        "    focus_points_smoothed : list of new focus points smoothed\n",
        "  \"\"\"\n",
        "  x_reversed = x[::-1]\n",
        "  if method == 'median':\n",
        "    focus_points_smoothed = pd.Series(focus_points).rolling(\n",
        "        window=window_size,\n",
        "        center=True\n",
        "    ).median()\n",
        "\n",
        "    # This will have NaN values\n",
        "    # on the first window_size//2 and last window_size//2 values\n",
        "    # We will consider the last 12 values\n",
        "    # by having windows centered on the left\n",
        "    focus_points_smoothed_right = pd.Series(focus_points).rolling(\n",
        "        window=window_size,\n",
        "        center=False\n",
        "    ).median()\n",
        "    #For the first 12 we will use  the the same reasoning\n",
        "    # and have a median centered on the right\n",
        "    # To do that we need to reverse the series\n",
        "    focus_points_smoothed_rev_right = pd.Series(x_reversed).rolling(\n",
        "        window=window_size,\n",
        "        center=False\n",
        "    ).median()\n",
        "    focus_points_smoothed_left = focus_points_smoothed_rev_right\n",
        "                                 .iloc[::-1]\n",
        "                                 .reset_index(drop=True)\n",
        "  if method == 'static':\n",
        "    focus_points_smoothed = pd.Series(focus_points).rolling(\n",
        "        window=window_size,\n",
        "        center=True\n",
        "    ).max()\n",
        "\n",
        "    focus_points_smoothed_right = pd.Series(focus_points).rolling(\n",
        "        window=window_size,\n",
        "        center=False\n",
        "    ).max()\n",
        "    focus_points_smoothed_rev_right = pd.Series(x_reversed).rolling(\n",
        "        window=window_size,\n",
        "        center=False\n",
        "    ).max()\n",
        "    focus_points_smoothed_left = focus_points_smoothed_rev_right\n",
        "                                 .iloc[::-1]\n",
        "                                 .reset_index(drop=True)\n",
        "  focus_points_smoothed[0:window_size//2] = \\\n",
        "    focus_points_smoothed_left[0:window_size//2]\n",
        "\n",
        "  focus_points_smoothed[- window_size//2 + 1 ::] = \\\n",
        "    focus_points_smoothed_right[- window_size//2 + 1 ::]\n",
        "\n",
        "  return focus_points_smoothed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvw11fxkUo3w"
      },
      "source": [
        "Choose below the focal method you want to execute and the window size. We highly recommend to play around with different values for focal method and for window_size to see how the curve of the focal points is smoothed out.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyei_a4wTNPw"
      },
      "outputs": [],
      "source": [
        "\n",
        "WINDOW_SIZE = 72 #@param {type:\"string\"}\n",
        "FOCAL_METHOD = \"static\" # @param [\"static\", \"median\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h4RH-EvQrej"
      },
      "source": [
        "The cell below will produce the new focus points smoothed out and also print the old focus points so you can see if the focus points are smoothed enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-aRrPBeTQLn"
      },
      "outputs": [],
      "source": [
        "# first non null in the list\n",
        "if not focus_points[0]:\n",
        "  # If first focus is null replace with the first one\n",
        "  focus_points[0] = next(x for x in focus_points if x)\n",
        "focus_points_smoothed = compute_new_focuses(focus_points,\n",
        "                                            method = FOCAL_METHOD,\n",
        "                                            window_size=WINDOW_SIZE)\n",
        "pd.Series(focus_points).plot()\n",
        "focus_points_smoothed.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxytxyrAe4J0"
      },
      "source": [
        "And the focal points are now smoother !\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWzmNNyHz0oy"
      },
      "source": [
        "## f. Create the smoothed Shorts video by aggregating the frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPBZL_OUGhqS"
      },
      "source": [
        "The section below will create a video that will be the aggregation of all the cropped frames centered on smoothed focus points and corresponding to a 9:16 format. \\\n",
        "We will use videoWriter from cv2 for this, which reads the file frame by frame and aggregates all frames in an output file which will be downloaded locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw9IfsR8TvK-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "# read the video\n",
        "\n",
        "cropped_no_audio = f'temporary_cropped_{base_name}_top_segment_' \\\n",
        "                   f'{SEGMENT_NUMBER}.mp4'\n",
        "\n",
        "cap = cv2.VideoCapture(segment)\n",
        "\n",
        "#Reframe frame by frame the video\n",
        "frame_index = 0\n",
        "video_output = None\n",
        "while True:\n",
        "  success, raw_image = cap.read()\n",
        "  if not success:\n",
        "      break\n",
        "  # Use the current frame's height\n",
        "  output_height = raw_image.shape[0]\n",
        "  # Adjusted width for 9:16 aspect ratio\n",
        "  output_width = int(output_height * (9/16))\u0026 ~1\n",
        "  timestamp_ms = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
        "  focus_x = round(focus_points_smoothed[frame_index])\n",
        "  if focus_x - output_width //2 \u003c 0:\n",
        "    # then we center on the left\n",
        "    x_start = 0\n",
        "    x_end = output_width\n",
        "  elif focus_x + output_width // 2 \u003e raw_image.shape[1]:\n",
        "    # then we center on the right\n",
        "    x_end =  raw_image.shape[1]\n",
        "    x_start = raw_image.shape[1] - output_width\n",
        "  else:\n",
        "    # otherwise we can center on focus_x\n",
        "    x_start = focus_x - output_width // 2\n",
        "    x_end = focus_x + output_width // 2\n",
        "  y_start = 0\n",
        "  y_end = output_height\n",
        "  cropped_frame = raw_image[y_start:y_end, x_start:x_end]\n",
        "  if not video_output:  # Check if 'out' has been initialized\n",
        "      video_output = cv2.VideoWriter(cropped_no_audio,\n",
        "                             fourcc,\n",
        "                             fps,\n",
        "                            (output_width, output_height))\n",
        "  # Write the cropped frame to the output video\n",
        "  video_output.write(cropped_frame)\n",
        "  frame_index+=1\n",
        "  # Adjust total as needed\n",
        "  with tqdm(total=len(focus_points), desc=\"Progress\") as pbar:\n",
        "      pbar.update(frame_index)\n",
        "\n",
        "  if cv2.waitKey(1) == ord('q'):\n",
        "      break\n",
        "\n",
        "\n",
        "# Release resources\n",
        "\n",
        "cap.release()\n",
        "video_output.release()  # Release the VideoWriter\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQk4farTIDoX"
      },
      "source": [
        "The created video is an aggregation of frames so there will be no audio. We will use the MovieEditor package that we used above to create the time cropped version to add the audio part of the cropped version into the AI generated Shorts video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELZEZLgnUqw7"
      },
      "outputs": [],
      "source": [
        "segment_audio = f'temporary_audio_{base_name}_top_segment_' \\\n",
        "                f'{SEGMENT_NUMBER}.aac'\n",
        "cropped_with_audio = f'{base_name}_top_segment_{SEGMENT_NUMBER}_' \\\n",
        "                     f'final_proposal.mp4'\n",
        "\n",
        "my_clip = mov.VideoFileClip(segment)\n",
        "\n",
        "my_clip.audio.write_audiofile(segment_audio, codec = 'aac')\n",
        "\n",
        "videoclip = mov.VideoFileClip(cropped_no_audio)\n",
        "duration = videoclip.duration\n",
        "audioclip = mov.AudioFileClip(segment_audio)\n",
        "audio_subclip = audioclip.subclip(0, duration)\n",
        "new_audioclip = mov.CompositeAudioClip([audio_subclip])\n",
        "videoclip.audio = new_audioclip\n",
        "videoclip.write_videofile(cropped_with_audio, audio_codec='aac')\n",
        "\n",
        "print(f\"All done! You have now created locally your short proposal: \"\n",
        "      f\"{cropped_with_audio}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWs7xyx_7eNu"
      },
      "source": [
        "Now that we have locally a short with the audio cropped vertically let's upload it to Google Cloud Storage in the output folder !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZO_uG0u7c5n"
      },
      "outputs": [],
      "source": [
        "upload_blob(BUCKET_NAME, cropped_with_audio,\n",
        "            f'output/{base_name}_top_segment_{SEGMENT_NUMBER}'\n",
        "            '_final_proposal.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKZuajpF-oO9"
      },
      "source": [
        "Congratulations ! You now have a Shorts video in your output folder of your Google Cloud Storage bucket, cropped vertically in a 9:16 format around the focus points and corresponding to the top segments in terms of Relative Retention performance. \\\n",
        "Now you can re execute this section for all the segments found in your video before wrapping up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M92SJTj9IGL8"
      },
      "source": [
        "# Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSp3NjU9IWkJ"
      },
      "source": [
        "**Only go to this step once you repeated part 2 for all segments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4_Rx3yLS2TM"
      },
      "source": [
        "Now that everything is done you will have locally a few files created in your folder.\n",
        " - the segments: VOD cut at the right time\n",
        " - The audio of the segment: ends with .aac\n",
        " - the segment cropped vertically without any audio\n",
        " - the segment cropped vertically with an audio\n",
        "\n",
        "The first 3 are temporary files necessary to make the study complete. We add here an optional step so you can delete those to avoid keeping temp files where your Colab is executed.\n",
        "\n",
        "This part should only be executed once you have treated all the segments looping through part 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKhRpkF7cGtU"
      },
      "outputs": [],
      "source": [
        "# Optional clean up step : Delete all the temporary files here\n",
        "\n",
        "def delete_local_files(b):\n",
        "    \"\"\"Deletes files starting with 'temporary_' within a given folder.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): The path to the folder containing the files.\n",
        "    \"\"\"\n",
        "\n",
        "    for filename in os.listdir():\n",
        "        if filename.startswith('temporary_') \\\n",
        "          or filename.startswith(base_name):\n",
        "            try:\n",
        "                os.remove(filename)\n",
        "                print(f\"Deleted: {filename}\")\n",
        "            except OSError as e:\n",
        "                print(f\"Error deleting {filename}: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "delete_local_files()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
