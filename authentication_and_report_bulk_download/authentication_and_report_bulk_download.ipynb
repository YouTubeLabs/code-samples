{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4GUt85D-oRx"
      },
      "source": [
        "# Download Reports from the YouTube Reporting API\n",
        "\n",
        "Use this Colab to download all available reports from the YouTube Reporting API to your Google Drive. You can choose the CMS that you want to download reports for. You will need to run this colab for each content owner that you have.\n",
        "\n",
        "In order to use this Colab you will need the following information:\n",
        "1. Your Content Owner ID. This is a 22 character ID that you can find in the URL to your content owner. **If you plan on downloading reports with revenue numbers, you will need to have permission to view revenue for the content owner.**\n",
        "\n",
        "2. Your Google Cloud Project ID. This can be found on the home page of the [Google Cloud Console](https://console.cloud.google.com/). If you don't already have a Google Cloud Project, there are instructions [here](https://developers.google.com/workspace/guides/create-project) on how to set that up.\n",
        "\n",
        "3. Choose if you want the reports to be decompressed when downloaded or left as a gzip format. This script downloads the reports as gzip files by default to make sure every size can be handled. We added a decompression parameter to decompress the gzip as a CSV directly after download if desired. By default it is set to `True` (box is checked), and the files are decompressed. Change this field to `False` (uncheck the box) if you do not want the reports to be decompressed.\n",
        "\n",
        "Make sure you have the YouTube Reporting API enabled on your Google Cloud Project. Instructions [here](https://support.google.com/googleapi/answer/6158841?hl=en\u0026scdeb=scapi\u0026sjid=5729436496856850200-EU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSzc_tNg-Q3Y"
      },
      "source": [
        "Please enter your Google Cloud Project ID and content owner ID in the fields below\n",
        "on the right.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwlyNAVtVzAi"
      },
      "outputs": [],
      "source": [
        "global GOOGLE_CLOUD_PROJECT_ID\n",
        "global CONTENT_OWNER_ID\n",
        "global DECOMPRESSION\n",
        "\n",
        "# Google Cloud Project ID\n",
        "GOOGLE_CLOUD_PROJECT_ID = 1234567   #@param {type:\"number\"}\n",
        "\n",
        "# Content Owner ID\n",
        "CONTENT_OWNER_ID = 'a1b2c3d4e5f6' #@param {type:\"string\"}\n",
        "\n",
        "# File Decompression settings\n",
        "DECOMPRESSION = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyRLAvJiNdaZ"
      },
      "source": [
        "## API Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA6pOQVEsiBK"
      },
      "source": [
        "The code below will authenticate you to the reporting API. It will ask you click on a link below, which opens in a new tab. This will prompt you to choose a Google account to authenticate with. After choosing the one which has access to your content owner, then you'll need to copy the code on the page and paste it below where it says \"Enter Authorization Code\".\n",
        "\n",
        "The authentication process gives rights to the following scopes:\n",
        "- yt-analytics-monetary.readonly\n",
        "- yt-analytics.readonly\n",
        "- cloud-platform.readonly\n",
        "\n",
        "\n",
        "\n",
        "Technically, it gives scopes of rights to the GCP project and creates virtually some credentials in a json file which will be the key of authentication to the API services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIuivvmnV4Pl"
      },
      "outputs": [],
      "source": [
        "\"\"\"YouTube Reporting API Authentication\"\"\"\n",
        "\n",
        "from googleapiclient import discovery\n",
        "SCOPES = (f'https://www.googleapis.com/auth/yt-analytics-monetary.readonly,'\n",
        "          f'https://www.googleapis.com/auth/yt-analytics.readonly,'\n",
        "          f'https://www.googleapis.com/auth/cloud-platform.read-only')\n",
        "API_SERVICE_NAME = 'youtubereporting'\n",
        "API_VERSION = 'v1'\n",
        "PROJECT = GOOGLE_CLOUD_PROJECT_ID\n",
        "\n",
        "!gcloud config set project $PROJECT\n",
        "!gcloud auth application-default login --scopes=$SCOPES\n",
        "\n",
        "def get_authenticated_service():\n",
        "    return discovery.build(API_SERVICE_NAME, API_VERSION)\n",
        "\n",
        "youtube_reporting = get_authenticated_service()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUxwgVSDbtdB"
      },
      "source": [
        "## Mounting your Google Drive\n",
        "\n",
        "The `mount()` function in Colab mounts your Google Drive account to the Colab runtime. This means that you can access all of the files in your Google Drive from within Colab, and you can also save files from Colab to your Google Drive.\n",
        "\n",
        "Please note this step will redirect to an authentication step, and it might take a few minutes if you have a lot of files in Drive.\n",
        "\n",
        "\n",
        "This step is optional: If you don't execute it, the files will be downloaded locally where the instance of the Colab stands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTzJmzDyVjGR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJOzq1p4WBkc"
      },
      "source": [
        "## Import the required packages for the report download\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dca5BwmbWEsW"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import requests\n",
        "import http.client\n",
        "import httplib2\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import optparse\n",
        "import urllib3\n",
        "import gzip\n",
        "import tempfile\n",
        "import shutil\n",
        "from types import prepare_class\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaIoBaseDownload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTWhWqa3-i6W"
      },
      "source": [
        "The following code retrieves all available bulk and system-managed reports in one go and saves them to your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0XZO3svuu0K"
      },
      "source": [
        "Below are all the functions that will call the different methods of the reporting API.\n",
        "\n",
        "The last 3 functions, `list_reporting_jobs()`, `retrieve_reports()` and `download_report()`, call respectively the jobs.list method to list jobs, jobs.reports.list to list the reports and then medial.download method to download the report files from the download url."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIVT6OOCuz28"
      },
      "outputs": [],
      "source": [
        "def remove_empty_kwargs(**kwargs):\n",
        "  \"\"\"Remove keyword arguments that are not set.\"\"\"\n",
        "  good_kwargs = {}\n",
        "  if kwargs is not None:\n",
        "    for key, value in kwargs.items():\n",
        "      if value:\n",
        "        good_kwargs[key] = value\n",
        "  return good_kwargs\n",
        "\n",
        "\n",
        "def list_reporting_jobs(youtube_reporting, **kwargs):\n",
        "  \"\"\"Return reporting jobs from the YouTube Reporting API's jobs.list()\n",
        "     method.\"\"\"\n",
        "  # Only include the onBehalfOfContentOwner keyword argument if the user\n",
        "  # set a value for the --content_owner argument.\n",
        "  kwargs = remove_empty_kwargs(**kwargs)\n",
        "  res = []\n",
        "  # Retrieve the reporting jobs for the user (or content owner).\n",
        "  results = youtube_reporting.jobs().list(**kwargs).execute()\n",
        "\n",
        "  if 'jobs' in results and results['jobs']:\n",
        "    jobs = results['jobs']\n",
        "    for job in jobs:\n",
        "      res.append({\n",
        "          'id':job['id'],\n",
        "          'name': job['name'],\n",
        "          'reportTypeId': job['reportTypeId']\n",
        "          })\n",
        "    return res\n",
        "  else:\n",
        "    print('No jobs found')\n",
        "\n",
        "\n",
        "def retrieve_reports(youtube_reporting, **kwargs):\n",
        "  \"\"\"Return reports created by a job with the YouTube Reporting API's\n",
        "     reports.list() method.\"\"\"\n",
        "  # Only include the onBehalfOfContentOwner keyword argument if the user\n",
        "  # set a value for the --content_owner argument.\n",
        "  kwargs = remove_empty_kwargs(**kwargs)\n",
        "\n",
        "  # Retrieve available reports for the selected job.\n",
        "  results = youtube_reporting.jobs().reports().list(**kwargs).execute()\n",
        "\n",
        "  if 'reports' in results and results['reports']:\n",
        "    return results['reports']\n",
        "\n",
        "\n",
        "# Call the YouTube Reporting API's media.download method to download the report.\n",
        "def download_report(youtube_reporting, report_url, local_file, decompression):\n",
        "  \"\"\"Download locally the report contained in the report url. Returns None.\n",
        "  The local file does not contain the suffix. If decompression = True then\n",
        "  the local file is in csv, otherwise in gzip\"\"\"\n",
        "  request = youtube_reporting.media().download(\n",
        "    resourceName=' '\n",
        "  )\n",
        "  request.uri = report_url\n",
        "  fh = tempfile.NamedTemporaryFile()\n",
        "  # Stream/download the report in a single request.\n",
        "  downloader = MediaIoBaseDownload(fh, request, chunksize=-1)\n",
        "  done = False\n",
        "  while done is False:\n",
        "    try:\n",
        "      status, done = downloader.next_chunk()\n",
        "    #add a throttle in case the limit of quota is exceeded\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "      if e.response.status_code == 429:\n",
        "        print('429 error detected. Sleeping for 60 seconds...')\n",
        "        time.sleep(60)\n",
        "        continue\n",
        "      else:\n",
        "        raise e\n",
        "    if status:\n",
        "      print(f'Download {int(status.progress() * 100)}%.')\n",
        "  print('Download Complete for '+ local_file)\n",
        "  # The file that is ready to be downloaded locally is fh.\n",
        "  # Is it a compressed file?\n",
        "  fh.seek(0)\n",
        "  # If file is already compressed, save it according to DECOMPRESSION variable.\n",
        "  try:\n",
        "    src = gzip.open(fh)\n",
        "    fh.seek(0)\n",
        "    if decompression:\n",
        "      local_file_csv = local_file + \".csv\"\n",
        "      with open(local_file_csv, 'wb') as f_out:\n",
        "        shutil.copyfileobj(src, f_out)\n",
        "    else:\n",
        "      local_file_gzip = local_file + \".csv.gz\"\n",
        "      with gzip.open(local_file_gzip, 'wb') as f_out:\n",
        "          shutil.copyfileobj(src, f_out)\n",
        "  # If file is not compressed, save it according to the DECOMPRESSION variable.\n",
        "  except OSError:\n",
        "    src = fh\n",
        "    if decompression:\n",
        "      local_file_csv = local_file + \".csv\"\n",
        "      with open(local_file_csv, 'wb') as f_out:\n",
        "        shutil.copyfileobj(src, f_out)\n",
        "    else:\n",
        "      local_file_gzip = local_file + \".csv.gz\"\n",
        "      with gzip.open(local_file_gzip, 'wb') as f_out:\n",
        "          shutil.copyfileobj(src, f_out)\n",
        "  src.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgZYkl0F22tD"
      },
      "source": [
        "As the functions above list the jobs, retrieve the reports and download the reports, we need to combine the 3 of them to bulk download the reports. The function below allows you to download all the reports currently available in the Reporting API for your content owner.\n",
        "\n",
        "An exhaustive list of the system-managed reports that are available in the API can be found [here](https://developers.google.com/youtube/reporting/v1/reports/system_managed/reports).\n",
        "\n",
        "An exhaustive list of the bulk reports that are available in the API can be found [here](https://developers.google.com/youtube/reporting/v1/reports/content_owner_reports)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT3rXln8u9LJ"
      },
      "outputs": [],
      "source": [
        "def bulk_download(youtube_reporting, content_owner_id, directory_path,\n",
        "                   decompression=True):\n",
        "  \"\"\"Bulk downloads all reports in a CMS and returns None. The path is only a\n",
        "  prefix, the suffix will depend on whether the file is compressed or not.\"\"\"\n",
        "  print('Authenticated successfully.')\n",
        "  try:\n",
        "\n",
        "    # First retrieve the jobs that correspond to all the reports you can download\n",
        "    jobs = list_reporting_jobs(youtube_reporting,\n",
        "                            onBehalfOfContentOwner=content_owner_id,\n",
        "                            includeSystemManaged=True)\n",
        "\n",
        "    if jobs:\n",
        "      print(f'Now treating {str(len(jobs))} jobs')\n",
        "      init_job = 0\n",
        "      for job in jobs:\n",
        "        init_job += 1\n",
        "        # Each job will contain a list of reports\n",
        "        reports = retrieve_reports(youtube_reporting,\n",
        "                          jobId=job['id'],\n",
        "                  onBehalfOfContentOwner=content_owner_id)\n",
        "        print(f'Treating Job {str(init_job)}')\n",
        "        if reports :\n",
        "          init_report = 0\n",
        "          for report in reports:\n",
        "            init_report+=1\n",
        "            print(f'Treating report {str(init_report)} of job {str(init_job)}')\n",
        "            if report:\n",
        "              # download each report in a compression .gz to make sure\n",
        "              # every size can be handled.\n",
        "              full_path = (f'{directory_path}{content_owner_id}_'\n",
        "                            f'{job[\"reportTypeId\"]}_{report[\"startTime\"]}')\n",
        "              try:\n",
        "                download_report(youtube_reporting, report['downloadUrl'],\n",
        "                              full_path, decompression)\n",
        "              except HttpError as e:\n",
        "                if e.resp.status == 429:\n",
        "                    print(\"HTTP error 429 (Rate Limit Exceeded) occurred. \"\n",
        "                          \"Waiting for 60 seconds...\")\n",
        "                    time.sleep(60)  # Sleep for 60 seconds\n",
        "                    continue\n",
        "                else:\n",
        "                  print(f'An HTTP error {e.resp.status} occurred:\\n{e.content}')\n",
        "\n",
        "        else:\n",
        "          print('No reports')\n",
        "    else:\n",
        "      print('No jobs')\n",
        "  except HttpError as e:\n",
        "    print(f'An HTTP error {e.resp.status} occurred:\\n{e.content}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhCV6uwodEcB"
      },
      "source": [
        "## Time to download the reports!\n",
        "\n",
        "If you have mounted your Google Drive, all the reports should be downloaded with the prefix `/content/gdrive` to see them in the same folder as the colab. All the files downloaded have to be the downloaded with the prefix `'/content/gdrive/My Drive/reports`.\n",
        "\n",
        "If you chose to export your reports to your Google Drive (default), the compressed reports will be exported into a `reports` folder in your drive. If you don't already have a `reports` folder, then the code below will create it for you.\n",
        "\n",
        "**If you did not choose to mount and export your reports to Google Drive:** the reports will be downloaded locally in the same location as the Colab. You need to set the `prefix` to `None`, so it won't point to your drive folder.\n",
        "\n",
        "The `bulk_download()` function downloads the reports as gzip files to make sure every size can be handled. We added a decompression parameter to decompress the report file to a CSV directly after download if desired. By default, it is set to `True` (`decompression=True`), and the files are decompressed. If you set `DECOMPRESSION` to false at the beginning of the script, the reports will be left as Gzip format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p-6weT0WRW8"
      },
      "outputs": [],
      "source": [
        "# Create a reports folder and change directory to the reports folder.\n",
        "\n",
        "youtube_reporting = get_authenticated_service()\n",
        "directory_path = '/content/drive/My Drive/reports/'\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "os.chdir(directory_path)\n",
        "prefix = directory_path\n",
        "\n",
        "bulk_download(youtube_reporting,\n",
        "              CONTENT_OWNER_ID,\n",
        "              directory_path,\n",
        "              decompression=DECOMPRESSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlMEQJXTNNUE"
      },
      "source": [
        "## Let's make sure it worked!\n",
        "\n",
        "Now that your reports have finished downloading, you can use the command below to view the reports that you've downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmclV9vQ7Jw-"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
